{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third-Order Letter Approximation Model Tasks\n",
    "### By Luke Corcoran\n",
    "### G00410404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary imports\n",
    "\n",
    "import os # For file operations\n",
    "import collections # For counting characters in files and creating a dictionary\n",
    "import random # For selecting random items from lists\n",
    "import json # For exporting model as a JSON file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Third-Order Letter Approximation Model\n",
    "### Step 1: Initialise Variables and Prepare to Process Files\n",
    "First, the variables `directory` and `keep` are initialised to store the text file directory and the characters to retain from the text files. The [Counter() function](https://github.com/ianmcloughlin/2425_emerging_technologies/blob/main/02_language_models.ipynb) from Python's 'collections' module is used to keep track of character counts across all files, and a list `cleanedFiles` stores the cleaned content of each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the text files\n",
    "directory = r'data\\utf8_english_works'\n",
    "\n",
    "# The characters to keep (ASCII, full stops, spaces).\n",
    "keep = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ .'\n",
    "\n",
    "# Initialise a Counter to store the frequency of each character across all files\n",
    "totalCounts = collections.Counter()\n",
    "    \n",
    "# Initialise a list to store cleaned files\n",
    "cleanedFiles = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read and Clean Text Files\n",
    "Next, the [listdir() function in the 'os' module](https://pytutorial.com/python-using-oslistdir-to-list-files-in-a-directory/) is used in a loop to iterate over each file in the folder. It checks if it ends with .txt and joins the file name and the directory path together using another function from the 'os' module, [path.join()](https://www.geeksforgeeks.org/python-os-path-join-method/), in order to access the file via the file path. The content of each text file is read with UTF-8 encoding using the [open()](https://stackoverflow.com/questions/491921/unicode-utf-8-reading-and-writing-to-files-in-python#:~:text=So%20by%20adding%20encoding%3D',of%20everything%20done%20in%20Python.) function. The text is converted to uppercase using the [upper()](https://www.programiz.com/python-programming/methods/string/upper) method. A [generator expression](https://www.pythonlikeyoumeanit.com/Module2_EssentialsOfPython/Generators_and_Comprehensions.html#:~:text=in%20Python%20code%3A-,Definition%3A,provided%20within%20the%20parenthetical%20statement.) filters and retains only the characters specified in the `keep` variable, and the [join()](https://discuss.python.org/t/quick-question-on-join/14756) method is used to merge these characters into a string. Finally, using the [append()](https://www.programiz.com/python-programming/methods/list/append) function, the cleaned file is appended to a list of cleaned files for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all files in the directory\n",
    "for fileName in os.listdir(directory):\n",
    "    if fileName.endswith('.txt'):\n",
    "        filePath = os.path.join(directory, fileName)\n",
    "            \n",
    "        # Open the file with UTF-8 encoding\n",
    "        with open(filePath, 'r', encoding='utf-8') as file:\n",
    "            # Read the whole file into a string.\n",
    "            english = file.read()\n",
    "\n",
    "            # Change everything to upper case.\n",
    "            english = english.upper()\n",
    "\n",
    "            # Remove unwanted characters.\n",
    "            cleaned = ''.join(c for c in english if c in keep)\n",
    "\n",
    "            # Append the cleaned file to the list of cleaned files\n",
    "            cleanedFiles.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract Main Content from Files and Initialise Word Counts\n",
    "In this step, a new loop is made which iterates over each cleaned file. Preamble and postamble is removed using both [find()](https://www.w3schools.com/python/ref_string_find.asp) and Python's [slicing function](https://www.w3schools.com/python/ref_func_slice.asp). If the specified start and end markers are found, it keeps only the text between them. It counts the characters in this cleaned text using the [Counter()](https://github.com/ianmcloughlin/2425_emerging_technologies/blob/main/02_language_models.ipynb) function and adds these counts to a total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each character\n",
    "for cleaned in cleanedFiles:\n",
    "    # Remove preamble and postamble.If find returns -1, the substring was not found.\n",
    "    start = cleaned.find('START OF THE PROJECT GUTENBERG EBOOK')\n",
    "    end = cleaned.find('END OF THE PROJECT GUTENBERG EBOOK')\n",
    "\n",
    "    # If the substrings are found, extract the main content.\n",
    "    if start != -1 and end != -1:\n",
    "        cleaned = cleaned[start:end]\n",
    "    else:\n",
    "        print(\"ERROR: Substrings not found in file:\", fileName)\n",
    "        \n",
    "    # Count the frequency of each character in the current file\n",
    "    counts = collections.Counter(cleaned)\n",
    "\n",
    "    # Update the total counts with the counts from the current file\n",
    "    totalCounts.update(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create List of Cleaned Files\n",
    "To demonstrate the program works so far, the frequency of each character is then printed using a loop that iterates over all the characters and their corresponding counts. The [items()](https://www.programiz.com/python-programming/methods/dictionary/items) function is used to retrieve the key-value pairs (characters and their counts) from `totalCounts`. The cleaned files are stored in a list called `cleanedList` so that they can be used to count the number of sequences in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'S': 136438\n",
      "'T': 193301\n",
      "'A': 171189\n",
      "'R': 122258\n",
      "' ': 442809\n",
      "'O': 159367\n",
      "'F': 46913\n",
      "'H': 136849\n",
      "'E': 265142\n",
      "'P': 36138\n",
      "'J': 2418\n",
      "'C': 49677\n",
      "'G': 43500\n",
      "'U': 60490\n",
      "'N': 147529\n",
      "'B': 33311\n",
      "'K': 16941\n",
      "'L': 87108\n",
      "'Y': 42416\n",
      "'I': 144493\n",
      "'M': 55608\n",
      "'D': 92967\n",
      "'W': 50034\n",
      "'.': 23533\n",
      "'V': 19577\n",
      "'X': 2706\n",
      "'Z': 1117\n",
      "'Q': 2769\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for char, count in totalCounts.items():\n",
    "    print(f\"'{char}': {count}\")\n",
    "\n",
    "# Store contents of cleaned files in a list in order to count the number of sequences later\n",
    "cleanedList = cleanedFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Creating Trigram Model\n",
    "Finally, using the [defaultdict()](https://www.geeksforgeeks.org/defaultdict-in-python/) function, a dictionary is initialised as the data structure to store the results; this is effective because dictionaries in Python use key-value pairs, which is perfect for storing each trigram as a key and its respective appearance count as a value.\n",
    "\n",
    "A new loop is made to iterate over each file's content and extract the trigrams that appear in each file by slicing the text from a particular index range using the [range()](https://www.w3schools.com/python/ref_func_range.asp)\n",
    " function, incrementing the count of each trigram in the dictionary as it goes. The total count for each trigram is arranged from highest to lowest using the [sorted()](https://www.freecodecamp.org/news/sort-dictionary-by-value-in-python/#:~:text=reverse%20with%20a%20value%20of,sorted%20dictionary%20in%20descending%20order.&text=You%20can%20see%20the%20output,That's%20the%20default.) function. The final result is a sorted dictionary that holds the contents of the trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' TH', 50120), ('THE', 42657), ('HE ', 33535), ('ED ', 19427), ('AND', 19167), ('ND ', 18886), (' AN', 18522), ('ING', 16298), (' OF', 15054), ('NG ', 14348), (' TO', 14170), ('OF ', 13953), ('TO ', 12595), ('ER ', 12564), (' IN', 12352), ('AT ', 12066), ('IS ', 11072), ('IN ', 10547), (' HE', 10223), ('RE ', 9472)]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the trigram counts \n",
    "trigramCounts = collections.defaultdict(int) \n",
    "\n",
    "# Iterate over each cleaned file's content\n",
    "for cleaned in cleanedList:\n",
    "    # Iterate over the cleaned text to extract trigrams \n",
    "    for i in range(len(cleaned)): \n",
    "        trigram = cleaned[i:i+3] # Creates trigram by slicing the cleaned text from index i to i+3\n",
    "        trigramCounts[trigram] += 1 # Increment the count of the trigram in the dictionary\n",
    "\n",
    "    # Sort the trigram counts from highest to lowest\n",
    "    trigramModel = sorted(trigramCounts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Only displays the top 20 trigrams for brevity\n",
    "print(trigramModel[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #1: Check to see if Preamble and Postamble is being Removed from Each File\n",
    "This test makes sure that the preamble and postamble is being removed from each file.\n",
    "\n",
    "The content before the preamble and postamble markers are checked using 'if' and 'in' statements - if they are present, the method returns false, and the test for the respective file will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for file 1 passed - preamble and postamble has been removed.\n",
      "Test for file 2 passed - preamble and postamble has been removed.\n",
      "Test for file 3 passed - preamble and postamble has been removed.\n",
      "Test for file 4 passed - preamble and postamble has been removed.\n",
      "Test for file 5 passed - preamble and postamble has been removed.\n"
     ]
    }
   ],
   "source": [
    "def ambleRemovedTest(cleanedFile):  \n",
    "    # Check that the content right before the preamble marker is not in the file\n",
    "    if \"Credits:\" in cleanedFile:\n",
    "        print(\"Test failed: Preamble not removed.\")\n",
    "        return False\n",
    "\n",
    "    # Check that the content right after the postamble marker is not in the file\n",
    "    if \"Updated editions will replace the previous one—the old editions will be renamed.\" in cleanedFile:\n",
    "        print(\"Test failed: Postamble not removed.\")\n",
    "        return False\n",
    "\n",
    "    # Triggers if all tests pass\n",
    "    return True\n",
    "\n",
    "# Initialise index for file number\n",
    "index = 1\n",
    "\n",
    "# Iterate over each cleaned file to test if preamble and postamble has been removed\n",
    "for cleaned in cleanedList:\n",
    "    result = ambleRemovedTest(cleaned)\n",
    "    if result == True:\n",
    "        print(\"Test for file\", index, \"passed - preamble and postamble has been removed.\")\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #2: Check to see if Valid Characters are Present\n",
    "This test makes sure that each file only contains valid ASCII characters.\n",
    "\n",
    "Each file is checked for the presence of non-ASCII characters using 'if' and 'not in' statements, looping over each character in the cleaned file. If any are present, the method will return false, and the test for the respective file will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for file 1 passed - all characters are valid ASCII characters.\n",
      "Test for file 2 passed - all characters are valid ASCII characters.\n",
      "Test for file 3 passed - all characters are valid ASCII characters.\n",
      "Test for file 4 passed - all characters are valid ASCII characters.\n",
      "Test for file 5 passed - all characters are valid ASCII characters.\n"
     ]
    }
   ],
   "source": [
    "def charValidityTest(cleanedFile):  \n",
    "    # Define the allowed characters\n",
    "    asciiCharacters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ .\"\n",
    "\n",
    "    # Check each character to ensure it's in the allowed set\n",
    "    for char in cleanedFile:\n",
    "        if char not in asciiCharacters:\n",
    "            print(f\"Test failed: Disallowed character '{char}' found in cleaned content.\")\n",
    "            return False\n",
    "\n",
    "    # Triggers if all tests pass\n",
    "    return True\n",
    "\n",
    "# Initialise index for file number\n",
    "index = 1\n",
    "\n",
    "# Iterate over each cleaned file to test if all characters are valid ASCII characters\n",
    "for cleaned in cleanedList:\n",
    "    result = charValidityTest(cleaned)\n",
    "    if result == True:\n",
    "        print(\"Test for file\", index, \"passed - all characters are valid ASCII characters.\")\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #3: Check to see if All Letters in Each File are Uppercase\n",
    "This test makes sure that every letter in each file is uppercase.\n",
    "\n",
    "\n",
    "The check for if all letters are uppercase is done using the [isupper()](https://www.geeksforgeeks.org/check-whether-the-given-character-is-in-upper-case-lower-case-or-non-alphabetic-character/) method, which returns true if all letters are uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for file 1 passed - all characters are uppercase.\n",
      "Test for file 2 passed - all characters are uppercase.\n",
      "Test for file 3 passed - all characters are uppercase.\n",
      "Test for file 4 passed - all characters are uppercase.\n",
      "Test for file 5 passed - all characters are uppercase.\n"
     ]
    }
   ],
   "source": [
    "def uppercaseTest(cleanedFile):      \n",
    "    # Check if all alphabetic characters are uppercase\n",
    "    if not cleanedFile.isupper():\n",
    "        print(\"Test failed: Not all alphabetic characters are uppercase.\")\n",
    "        return False\n",
    "\n",
    "    # Triggers if all tests pass\n",
    "    return True\n",
    "\n",
    "# Initialise index for file number\n",
    "index = 1\n",
    "\n",
    "# Iterate over each cleaned file to test if all characters are uppercase\n",
    "for cleaned in cleanedList:\n",
    "    result = uppercaseTest(cleaned)\n",
    "    if result == True:\n",
    "        print(\"Test for file\", index, \"passed - all characters are uppercase.\")\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #4: Check for if Trigram Models contain Valid Trigrams\n",
    "This test checks that each Trigram model only has valid trigrams (with three characters). \n",
    "\n",
    "It takes the trigram model as an argument and iterates over each trigram, checking if each one has exactly three characters. For any trigram that does not have exactly three characters, it will return False, printing out the invalid trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed - trigram model contains valid trigrams.\n"
     ]
    }
   ],
   "source": [
    "def trigramModelTest(trigramModel):     \n",
    "    # Check each trigram in the model\n",
    "    for trigram, count in trigramModel:\n",
    "        # Check if each trigram is exactly 3 characters\n",
    "        if len(trigram) != 3:\n",
    "            print(f\"Test failed: Trigram '{trigram}' is not 3 characters long.\")\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "# Run the test with this cleaned text\n",
    "result = trigramModelTest(trigramModel)\n",
    "if result == True:\n",
    "    print(\"Test passed - trigram model contains valid trigrams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Third Order Letter Approximation Generation\n",
    "\n",
    "### Step 1: Initialise Starting Trigrams and Weights\n",
    "To begin this task, trigrams beginning with TH are found and weights are created using [list comprehension](https://chatgpt.com/share/670583e8-8ca0-800f-bddd-9e8e27b62db1), with the weights being based off each trigram's reoccurence in the model. Both the trigrams beginning with TH along with their odds of being picked are then displayed using the [zip()](https://www.geeksforgeeks.org/zip-in-python/) function to merge the corresponding lists of trigrams and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams: | Odds of being chosen:\n",
      "THE       | 42657/67788\n",
      "THA       | 8266/67788\n",
      "TH        | 5632/67788\n",
      "THI       | 5589/67788\n",
      "THO       | 2667/67788\n",
      "THR       | 1349/67788\n",
      "THU       | 353/67788\n",
      "THY       | 279/67788\n",
      "THS       | 254/67788\n",
      "TH.       | 214/67788\n",
      "THT       | 100/67788\n",
      "THL       | 74/67788\n",
      "THW       | 65/67788\n",
      "THF       | 57/67788\n",
      "THD       | 51/67788\n",
      "THH       | 50/67788\n",
      "THM       | 33/67788\n",
      "THP       | 23/67788\n",
      "THC       | 18/67788\n",
      "THN       | 13/67788\n",
      "THB       | 12/67788\n",
      "THQ       | 12/67788\n",
      "THG       | 10/67788\n",
      "THK       | 5/67788\n",
      "THJ       | 3/67788\n",
      "THV       | 2/67788\n"
     ]
    }
   ],
   "source": [
    "# Use list comprehension to find trigrams that start with 'TH'\n",
    "thKeys = [key for key, value in trigramModel if key.startswith('TH')]\n",
    "\n",
    "# Create weights based on reoccurrence of trigrams in the model\n",
    "weights = [value for key, value in trigramModel if key in thKeys]\n",
    "\n",
    "# Calculate the total weight\n",
    "totalWeight = sum(weights)\n",
    "\n",
    "# Print trigrams and weights alongside each other as key-value pairs with total weight as denominator\n",
    "print(\"Trigrams: | Odds of being chosen:\")\n",
    "for key, weight in zip(thKeys, weights):\n",
    "    print(f\"{key}       | {weight}/{totalWeight}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialise Start of 10,000 Character String with Random Trigram\n",
    "A trigram is then picked at random using the [random.choices()](https://github.com/ianmcloughlin/2425_emerging_technologies/blob/main/02_language_models.ipynb) function, [which bases its selection off weights](https://pynative.com/python-weighted-random-choices-with-probability/); with the more reoccuring trigrams having a higher chance of being selected such as 'THE', 'THA' etc. Using the [str()](https://www.w3schools.com/python/ref_func_str.asp)\n",
    " function the chosen trigram is then converted to a string, and the beginning of the 10,000 character string is initialised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen trigram to begin the string: THS\n"
     ]
    }
   ],
   "source": [
    "# Pick a trigram based on the weights, using [0] to extract the first element of the list \n",
    "chosenTrigram = random.choices(thKeys, weights)[0]\n",
    "\n",
    "# Convert the randomly chosen trigram to a string\n",
    "chosenTrigram = str(chosenTrigram)\n",
    "\n",
    "# Print the beginning of the 10,000 character string\n",
    "print(\"Chosen trigram to begin the string:\", chosenTrigram)\n",
    "\n",
    "# Initialise the generated string with the chosen trigram\n",
    "generatedString = chosenTrigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate More Characters to Add to the String\n",
    "Finally, the string needs more characters added to it to reach a length of 10,000 characters. In order to do this, a loop is created that adds characters to the string. Python's [string slicing](https://pythonexamples.org/python-string-get-last-n-characters/) function is used to get the last two characters of the string. [List comprehension](https://chatgpt.com/share/670583e8-8ca0-800f-bddd-9e8e27b62db1) is then used to both find trigrams in the model that begin with these two characters and create weights based off their reoccurence, and the [random.choices()](https://www.w3schools.com/python/ref_random_choices.asp) function is used to randomly select one of the third letters of those trigrams based off their weights. For each third letter that is selected, it is added to the string until it reaches 10,000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop until the string reaches the desired length (10,000 characters)\n",
    "while len(generatedString) < 10000:\n",
    "    # Get the last two characters of the current generated string\n",
    "    lastTwoChars = generatedString[-2:]\n",
    "        \n",
    "    # Use list comprehension to find trigams that start with the last two characters\n",
    "    possibleTrigrams = [key for key, value in trigramModel if key.startswith(lastTwoChars)]\n",
    "        \n",
    "    # Create weights based on reoccurrence of trigrams in the model\n",
    "    weights = [value for key, value in trigramModel if key in possibleTrigrams]\n",
    "        \n",
    "    # Pick a trigram based on the weights\n",
    "    chosenTrigram = random.choices(possibleTrigrams, weights)[0]\n",
    "        \n",
    "    # Add the third character of the chosen trigram to the generated string\n",
    "    generatedString += chosenTrigram[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #1: Check to see that the String is 10,000 Characters Long.\n",
    "This test verifies that the generated string reaches exactly 10,000 characters.\n",
    "\n",
    "If successful, it outputs a success message along with the first 100 characters for quick verification. If the string is not 10,000 characters, it prints an error message showing the actual length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed: Generated String (first 100 characters): THS OF WOR STIM THAREELIAT DAME DOWELFAT BAW HIPWAS SHE OF HE OF THERLESS THE BY OF TOICHAL BE SUP A\n"
     ]
    }
   ],
   "source": [
    "# Ensure the generated string is exactly 10,000 characters long\n",
    "if len(generatedString) == 10000:\n",
    "    # Print the first 100 characters for verification\n",
    "    print(\"Test Passed: Generated String (first 100 characters):\", generatedString[:100])\n",
    "else:\n",
    "    # Return the generated string with an error message\n",
    "    print(\"Test Failed: The generated string is not 10,000 characters, it is only\", len(generatedString), \"characters long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #2: Check for Valid Characters in Generated String\n",
    "Checks to ensure that the generated string contains only allowed characters (ASCII characters).\n",
    "\n",
    "Creates two sets comprised of both the characters in the generated string and characters that are allowed in the string, finding which characters are invalid by [subtracting the valid character set from the string character set](https://docs.python.org/3/library/stdtypes.html#set). Any characters in the difference are stored in `disallowedChars`, and are invalid characters; they will cause the test to fail if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed: All characters in the string are valid.\n"
     ]
    }
   ],
   "source": [
    "# Create sets with characters in the generated string and valid (ASCII) characters\n",
    "asciiChars = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ .\")\n",
    "charsInString = set(generatedString)\n",
    "\n",
    "# Find any disallowed characters by using set difference\n",
    "disallowedChars = charsInString - asciiChars\n",
    "\n",
    "if not disallowedChars:\n",
    "    print(\"Test Passed: All characters in the string are valid.\")\n",
    "else:\n",
    "    print(\"Test Failed: Found disallowed character(s):\", disallowedChars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Analyze Your Model\n",
    "\n",
    "### Step 1: Prepare English Words List and Split the Generated String\n",
    "To begin this task, the path to a file called words.txt, which contains every word in the English dictionary, is stored in the variable `wordsFile`. The [split()](https://stackoverflow.com/questions/6181763/converting-a-string-to-a-list-of-words) function is used to split the `words.txt` file into a list of individual words, and [set()](https://chatgpt.com/share/671c3805-17b0-800f-945a-29c947e9b23e) is used to remove duplicates, creating a complete set of English words. In order to split the 10,000 character string into substrings that could possibly be words for future comparison, the `split()` function is used once more, with the result being stored in `wordsInString`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the path to the words file to a variable \n",
    "wordsFile = r'data\\english_words\\words.txt'\n",
    "\n",
    "# Read the list of English words from the file, creating a set of english words\n",
    "with open(wordsFile, 'r') as file:\n",
    "    englishWords = set(file.read().split())\n",
    "\n",
    "# Split the extended string into individual substrings\n",
    "wordsInString = generatedString.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the Percentage of English Words in the String\n",
    "To calculate the percentage of English words, a [generator expression](https://www.pythonlikeyoumeanit.com/Module2_EssentialsOfPython/Generators_and_Comprehensions.html#:~:text=in%20Python%20code%3A-,Definition%3A,provided%20within%20the%20parenthetical%20statement.) is used to iterate over the contents of both the English words list and the generated string, allowing us to count the number of actual English words in the generated string. The percentage of valid English words relative to the total number of words in the string is then calculated using basic math and the result is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of valid English words: 34.44%\n"
     ]
    }
   ],
   "source": [
    "# Count the number of English words. The generator yields 1 for every word found in the set of English words.\n",
    "validWordCount = sum(1 for word in wordsInString if word in englishWords)\n",
    "\n",
    "# Calculate the percentage of valid English words\n",
    "totalWords = len(wordsInString)\n",
    "percentage = (validWordCount / totalWords) * 100\n",
    "\n",
    "# Print the percentage of valid English words\n",
    "print(f\"Percentage of valid English words: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #1: Ensure English Word Set from words.txt Contains No Non-Alphabetic Entries\n",
    "This test checks that the set of English words loaded from words.txt contains only alphabetic words.\n",
    "\n",
    "[List comprehension](https://realpython.com/list-comprehension-python/#using-conditions-in-list-comprehensions) is used to check if the words in `englishWords` are non alphabetical, using the [isalpha()](https://www.programiz.com/python-programming/methods/string/isalpha) function for comparison. If any non-alphabetical words are present in `nonAlphabeticalWords`, the test will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed: English words set contains alphabetic entries\n"
     ]
    }
   ],
   "source": [
    "# List comprehension to find non-alphabetic words in the set of English words\n",
    "nonAlphabeticWords = [word for word in englishWords if not word.isalpha() and word == \"-\" and word == \"'\"]\n",
    "\n",
    "# Check if the list of non-alphabetic words is empty\n",
    "if not nonAlphabeticWords:\n",
    "    print(\"Test Passed: English words set contains alphabetic entries\")\n",
    "else:\n",
    "    print(\"Test Failed: English words set contains non-alphabetic entries:\", nonAlphabeticWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test #2: Test for Presence of Common English Words in words.txt\n",
    "Checks that a list of common English words (such as \"THE\", \"OF\", \"AND\" etc.) are present in the englishWords set, verifying the validity of the file.\n",
    "\n",
    "[List comprehension](https://realpython.com/list-comprehension-python/#using-conditions-in-list-comprehensions) is used to filter the words in `commonWords` that are found in `englishWords`. The test compares the length of the resulting list, `commonWordsFound`, with the length of `commonWords`. If all words from commonWords are found in englishWords, the test passes. Otherwise, it prints the missing words by [finding the difference in both sets](https://docs.python.org/3/library/stdtypes.html#set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed: All words in the common words list are present in the set of English words.\n"
     ]
    }
   ],
   "source": [
    "# List of common English words\n",
    "commonWords = [\"THE\", \"OF\", \"AND\", \"TO\", \"IN\", \"THAT\", \"IS\", \"IT\", \"FOR\", \"ON\", \n",
    "                        \"WITH\", \"AS\", \"AT\", \"BY\", \"THIS\"]\n",
    "\n",
    "# Check presence of common words in words.txt\n",
    "commonWordsFound = [word for word in commonWords if word in englishWords]\n",
    "\n",
    "# Test passes if all common words are found in the set of English words\n",
    "if len(commonWordsFound) == len(commonWords):\n",
    "    print(\"Test Passed: All words in the common words list are present in the set of English words.\")\n",
    "else:\n",
    "    print(\"Test Failed: Words missing from the common words list:\", set(commonWords) - set(commonWordsFound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Exporting Model as JSON\n",
    "### Step 1: Creating Method to Convert Model to JSON File\n",
    "A simple method named `convertModelToJSON` is used to create a JSON file using the [json.dump()](https://www.geeksforgeeks.org/convert-python-list-to-json/) function, which is ideal for converting lists to JSON files. In this case, it will be used to convert the trigram model, which is in list format, into the required JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertModelToJSON(trigramModel):\n",
    "    # Export the trigram model as a JSON file\n",
    "    with open('trigrams.json', 'w') as file:\n",
    "        json.dump(trigramModel, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Export the trigram model as a JSON file\n",
    "The trigram model which was created in Task 1 is passed in as an argument to `convertModelToJSON`, exporting the model as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the trigram model to a JSON file\n",
    "convertModelToJSON(trigramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook aims to provide a comprehensive implementation of a third-order letter approximation model. By running through each task step-by-step, it demonstrates the process of cleaning and preparing text data, generating a trigram model to capture character sequences, and using this model to create a string of text based on probability. The final part of the program compares this string of text with words from the English dictionary to determine what percentage of the generated string is valid English words. Finally, the trigram model is exported as a JSON file. This project highlights key computational linguistics techniques and shows the importance of cleaning data, designing algorithms, and carefully evaluating results to build reliable language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
